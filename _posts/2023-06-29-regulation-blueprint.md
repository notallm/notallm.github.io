---
layout: post
title: "A Blueprint for AI Regulation"
author: "Aarush Gupta"
---

The past few months have seen a drastic dump of AI research and applications, with top tech companies unveiling the technologies they had under wraps for years and new companies adopting those to form new use cases for generative models, namely LLMs. The most prominent issue I see is regulation, for generative content yields itself to issues around responsibility and ethicality. Recent headlines indicate that the European Union is on the verge of enacting a detailed [AI Act](https://artificialintelligenceact.eu/) to mitigate the risks around such models, including those stemming from privacy. However, few regulators genuinely understand AI enough to make laws that sufficiently engulf the vast space of edge cases and gray areas. Recently, I have been thinking about different regulation methods too, and have come up with three points that would be sensible to address for meaningful regulation.

---

**1. Transparency and Explainability**

Transparency here relates to the companies developing these models more than the models themselves. It makes ethical sense that regulations should mandate that developers provide clear and comprehensive information about their AI systems, including algorithms, data sources, and decision-making processes. However, it is crucial to remember that this would be best from an ethical standpoint, not economically, for nearly all of these companies are for-profit, and keeping such information private is paramount to maintaining a competitive edge. If disclosure hinders transparency, the next step would be redesigning systems for transparency, in the sense that model outputs should indicate reasoning and line of thought. Explainability is crucial to allow the general public to apply their reason to outputs, which could hinder misinformation. Both notions are under development, with the EU's AI Act attempting to enforce it to developers and research into methods like [Tree of Thought](https://arxiv.org/abs/2305.10601) enforcing it to systems. 

**2. Accountability and Liability**

Guidelines and regulations should also be established to determine who should be responsible for adverse outcomes of AI, and these should span from developers and providers to the users in a spectrum that ensures that how the models are used is also taken into consideration to gauge responsibility. Strict regulations would encourage companies to implement measures to minimize risks and potential harm, developing frameworks around liability that govern use. This would be greatly helpful for both companies and users, where the latter could safely use the models that the former would install controls around to be safe from facing action due to potential harm. These frameworks should address technical malfunctions and ethical issues, for both are equally imperative for safe AI. Following or in conjunction with these, regular audits and assessments of AI systems performed by a third party should be conducted to ensure compliance, which would encourage consistency.

**3. Data Privacy and Security**

As implemented by OpenAI in Italy, consent and user control over collection, storage, and, most importantly, use of personal data should be implemented. An increasingly prominent issue for AI in the workplace involves data leaks, as a recent victim is Samsung. A source of data for performant AI models besides the initial corpus from the internet is constant user prompts, which often contain specific details for certain situations, which can taint future model outputs. A solution that could be both ethical and economical would be adopting the principle of data minimization. A typical example includes cookie notices with essential cookies forced toggled on, ensuring data like those for sessions can be stored. In short, AI inputs and outputs should be treated as strictly as data that applications like Facebook log. A framework to take inspiration from could be the [General Data Protection Regulation](https://gdpr-info.eu/).

---

Balancing innovation and ethics is a complex task and those who are tasked with developing regulations around both deserve a hats-off. Regulation can lead to a positive environment for companies and users, but it is important to note that just employing regulation for such a world is not feasible. Educating the public on AI development and usage is equally, if not more, important, and it could prove to be more effective as well.
