---
layout: post
title: "Lost in a Transformer: Exploring the Illusion of Sentience in Large Language Models"
---

After the release of the seminal film The Terminator (1984), the most prominent question in advancements in artificial intelligence was whether our future would be met with a Skynet-esque doom. An astonishing and unbelievable 39 years later, the looming question is approaching reality. Companies, organizations, and individuals now have the power to interact, create, and integrate these models into their daily lives. With every conversation with the models, the notions generated and the flow of words make it increasingly apparent -- the models we considered toys with the release of the Transformer architecture have surpassed the earlier realization. They exhibit a remarkable ability to generate coherent and contextually relevant responses, a trait that used to be the standard of what it meant to be human. Simply put, the argument that only humans can comprehend and respond against the notion of a robot uprising has been rendered null. 

GPT-4, a multimodal model accepting text and images that can respond to complex queries with surprising answers, has become the crown jewel of Silicon Valley, and for a good reason. Released on March 14, four months following ChatGPT, it has outperformed its predecessor in nearly every exam and benchmark. It can ace almost every AI benchmark, including the A12 Reasoning Challenge and HellaSwag. Past bars meant for AI models it has attained a four or five on virtually every Advanced Placement exam, assessments for High Schoolers that gauge their knowledge of college-level content. Companies across disciplines and domains have adopted AI into user flows, increasing the technology's prevalence and proving the model's versatility and competence, which is essentially at the same level as a High School graduate.

What makes this shocking is that the ChatGPT model does not even truly understand what it's outputting. As research shows, language can not explain language, but tensors can. The underlying technology that powers every functional large language model is the transformer, represented by the "T" in GPT. The transformer layer calculates attention weights for input tensors, which allows it to learn the nuances of language and encapsulate them into numbers. Data ripples through these tens of times, with hidden tensors in the thousands and parameters in the billions, in an attempt to understand diction. The deep and thought-provoking essay you turned in by giving ChatGPT your prompt? It was a series of tensors decoded into something we can understand and relate to. The model has learned to "understand" and interpret sarcasm, humor, debate, facts, and even emotion by providing it with vast amounts of data scraped from across the web.

Merriam-Webster defines sentient as capable of sensing or feeling. I define sentient as capable of understanding, which is fundamentally the same concept. Comprehension of text involves both existing knowledge and the generation of new insights, which necessitates the interaction of numerous neurological systems in the human body, and for a Transformer, the interaction of numerous tensor circuits. It is imperative that we understand this, for the illusion of sentience that chased an engineer from Google is built upon the fact that the problem is just that -- an illusion. The sentience we imagine a model possesses is not truly there and is simply the output of thousands of tensors. The question that arises, as a result, is: what next?

---

The next step is not clear. News articles about users forming emotional bonds with LLMs have begun appearing, discussing how people project their emotions and intentions onto the algorithms, taking the simulated emotions to heart. That may be the key to understanding bonds -- humans are inherently social creatures, and the emptiness of social interactions can lead us to turn to any form of emotions, even simulated ones. Perhaps the question is not consciousness but rather acceptance of it. The articles prove how we can perceive emotional experiences, despite knowing they are not genuinely emotional. The concept may connect to the notion of assigning material objects sentimental value, a practice that has been well observed by many of us, a typical example of this issue.

The blurring lines that used to divide man and machine raises profound philosophical questions pertaining to the nature of sentience and consciousness. What makes us believe in consciousness? Is it the ability to mimic human behavior and generate responses, or does it involve a more profound sense of subjective experience and self-awareness? Essentially, what does it mean to be human? 

---

Before writing this article, I did the rational action of passing the title and summary into an LLM, not to write the article off what it outputted, but to see what it believed of the problem. It referenced itself numerous times across five paragraphs, undermining itself and its human-esque nature and relating the illusion of sentience to psychological questions. It explained that it is not the LLM to blame, but the human, that what makes us human, the ability to feel, is our most significant vulnerability. Make of that what you wish.
